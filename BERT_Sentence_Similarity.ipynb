{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BertModel\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    sentence1, sentence2 = sys.argv[1:sys.argv.index(\",\")], sys.argv[sys.argv.index(\",\")+1:]\n",
        "\n",
        "    sentence1, sentence2 = \" \".join(sentence1), \" \".join(sentence2)\n",
        "    sentences = [sentence1, sentence2]\n",
        "\n",
        "    # Tokenize each sentence\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "    sentences_tokenized = [tokenizer.tokenize(s) for s in sentences]\n",
        "    MAX_LENGTH = max([len(sentences_tokenized[0]), len(sentences_tokenized[1])])\n",
        "\n",
        "    # Generate IDs of each token and add padding to sentences smaller than given threshold\n",
        "    ids = [tokenizer.convert_tokens_to_ids(t) for t in sentences_tokenized]\n",
        "    # Pad the tokens\n",
        "    ids = np.asarray([np.pad(i, (0, MAX_LENGTH-len(i)), mode='constant') for i in ids])\n",
        "\n",
        "\n",
        "    # Generate the masks\n",
        "    amasks = np.asarray([[float(i>0) for i in seq] for seq in ids])\n",
        "\n",
        "    # Get the output from the model\n",
        "    output = model.forward(torch.tensor(ids), torch.tensor(amasks))\n",
        "\n",
        "\n",
        "    # Now create two vector representations for each sentence\n",
        "    # pool_vectors (2x768) is the vector generated by perofrming max-pooling over the hidden states\n",
        "    pool_vectors = torch.max(output[0],dim=1)[0]   # Fill up this '----------' section\n",
        "    # cls_vectors (2x768) is the vector generated by taking the CLS token\n",
        "    cls_vectors = output[0][:, 0, :]     # Fill up this '----------' section\n",
        "\n",
        "    # Compute the cosine similarity\n",
        "    cosine_pooling = torch.sum(pool_vectors[0] * pool_vectors[1]) / (torch.norm(pool_vectors[0]) * torch.norm(pool_vectors[1]))   # Fill up this '----------' section\n",
        "    cosine_cls = torch.sum(cls_vectors[0] * cls_vectors[1]) / (torch.norm(cls_vectors[0]) * torch.norm(cls_vectors[1])) # Fill up this '----------' section\n",
        "\n",
        "    # Finally print out the values\n",
        "    print(np.round(cosine_pooling.item(), 2), np.round(cosine_cls.item(), 2))"
      ],
      "metadata": {
        "id": "FwiX4P-S7CJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}