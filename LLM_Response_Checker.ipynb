{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtItGUVcaFpF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05c50ac-06d2-4d8c-f693-3e37a1204eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NO\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import sys\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import re\n",
        "\n",
        "transformers.logging.set_verbosity_error()\n",
        "transformers.utils.logging.disable_progress_bar()\n",
        "\n",
        "\n",
        "def clean_decoded_output(decoded_output):\n",
        "    # clean the output from LLM and return\n",
        "    cleaned = decoded_output.strip().upper()\n",
        "    if 'YES' in cleaned:\n",
        "        return 'YES'\n",
        "    elif 'NO' in cleaned:\n",
        "        return 'NO'\n",
        "    else:\n",
        "        return 'NO'\n",
        "\n",
        "def llm_function(model,tokenizer,a,b):\n",
        "   he model (Flan-T5-XL) and tokenizer is already initialized. Do not modify that section.\n",
        "\n",
        "    # Step 1: Create prompt\n",
        "    prompt = f\"Question: {a}\\nAnswer: {b}\\nIs this answer correct? Respond with YES or NO.\"\n",
        "\n",
        "    # Step 2: Tokenize prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Step 3: Generate output from model\n",
        "    outputs = model.generate(**inputs, max_new_tokens=5)\n",
        "\n",
        "    # Step 4: Decode output\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Step 5: Clean and return output\n",
        "    cleaned_output = clean_decoded_output(decoded_output)\n",
        "\n",
        "    return cleaned_output\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_data_one = sys.argv[1].strip()\n",
        "    input_data_two = sys.argv[2].strip()\n",
        "\n",
        "\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    a = input_data_one\n",
        "    b = input_data_two\n",
        "\n",
        "    torch.manual_seed(42)\n",
        "    out = llm_function(model,tokenizer,a,b)\n",
        "    print(out.strip())\n",
        "\n"
      ]
    }
  ]
}